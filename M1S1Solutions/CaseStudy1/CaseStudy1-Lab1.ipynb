{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1\n",
    "## Case Study 1\n",
    "### Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "We use the credit card data set for this problem\n",
    "\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universit√© Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n",
    "\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "### Objective\n",
    "To use the kNN classifier and explore the ramifications, for a highly unbalanced dataset.\n",
    "\n",
    "#### Data Set Information:\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. \n",
    "\n",
    "### Data Attributes\n",
    "Due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA (to be discussed in future sessions), the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise\n",
    "\n",
    "### Predicted attribute\n",
    "Class: Fraud or Genuine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this dataset is rather large, so we suggest you use the first 10,000 records extracted and given in 10kcc.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,  -1.35980713e+00,  -7.27811733e-02, ...,\n",
       "         -2.10530535e-02,   1.49620000e+02,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.19185711e+00,   2.66150712e-01, ...,\n",
       "          1.47241692e-02,   2.69000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,  -1.35835406e+00,  -1.34016307e+00, ...,\n",
       "         -5.97518406e-02,   3.78660000e+02,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  1.50040000e+04,   1.22845486e+00,   4.94878251e-02, ...,\n",
       "          1.22771223e-02,   1.21800000e+01,   0.00000000e+00],\n",
       "       [  1.50080000e+04,  -9.71733659e-01,   7.44625377e-01, ...,\n",
       "          3.12717294e-02,   4.05000000e+00,   0.00000000e+00],\n",
       "       [  1.50100000e+04,  -1.52966552e+00,   1.47586981e+00, ...,\n",
       "          7.84608813e-03,   4.05000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rec = pd.read_csv(\"10kcc.csv\")\n",
    "rec.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "def dist(a, b):\n",
    "    sqSum = 0\n",
    "    for i in range(len(a)):\n",
    "        sqSum += (a[i] - b[i]) ** 2\n",
    "    return math.sqrt(sqSum)\n",
    "# ------------------------------------------------ #\n",
    "# We are assuming that the label is the last field #\n",
    "# If not, munge the data to make it so!            #\n",
    "# ------------------------------------------------ #\n",
    "def kNN(k, train, given):\n",
    "    distances = []\n",
    "    for t in train:\n",
    "        distances.append((dist(t[:-1], given), t[-1]))\n",
    "    distances.sort()\n",
    "    return distances[:k]\n",
    "\n",
    "def kNN_classify(k, train, given):\n",
    "    tally = collections.Counter()\n",
    "    for nn in kNN(k, train, given):\n",
    "        tally.update(nn[-1])\n",
    "    return tally.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** :: Split the dataset into for training and testing. Use K = 7 and Euclidean distance. Find the accuracy (as percentage) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def split_random(input_data):\n",
    "    TRAIN_TEST_RATIO = 0.8\n",
    "    picker = list(range(input_data.shape[0]))\n",
    "    random.shuffle(picker)       ### randomly shuffle the data\n",
    "    trainMax = int(len(picker) * TRAIN_TEST_RATIO)\n",
    "    train = []\n",
    "    test = []\n",
    "    for pick in picker[:trainMax]:\n",
    "        train.append(list(input_data.values[pick]))         ### select 80% of data to be used as training set\n",
    "    for pick in picker[trainMax:]:\n",
    "        test.append(list(input_data.values[pick]))       ### select 20% of data to be used as test set\n",
    "    return train , test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = split_random(rec)\n",
    "len(train)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** :: Repeat the above (creating random partitions and evaluating the performance) 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
